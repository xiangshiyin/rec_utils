{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the NCF module under folder [cf_ec2](../cf_ec2) with ml-1m dataset, save the best model (using integrated modules with compile and fit components, with gmf and mlp pretrain)\n",
    "\n",
    "#### 4/18/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import (\n",
    "    Adam,\n",
    "    Adamax,\n",
    "    Adagrad,\n",
    "    SGD,\n",
    "    RMSprop\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, \n",
    "    Input,\n",
    "    Flatten, \n",
    "    Multiply, \n",
    "    Concatenate,\n",
    "    Dense\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from cf_ec2 import (\n",
    "    GMF,\n",
    "    MLP,\n",
    "    NCF,\n",
    "    Data,\n",
    "    evaluation,\n",
    "    evaluation_grouped\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/ml-1m-all-train.csv',sep=',',header=0,names=['user','item','rating','event_ts'])\n",
    "test = pd.read_csv('../data/ml-1m-all-test.csv',sep=',',header=0,names=['user','item','rating','event_ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, (250088, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.user.nunique(), test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, (750121, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.user.nunique(), train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>event_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3186</td>\n",
       "      <td>4.0</td>\n",
       "      <td>978300019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1721</td>\n",
       "      <td>4.0</td>\n",
       "      <td>978300055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1270</td>\n",
       "      <td>5.0</td>\n",
       "      <td>978300055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating   event_ts\n",
       "0     1  3186     4.0  978300019\n",
       "1     1  1721     4.0  978300055\n",
       "2     1  1270     5.0  978300055"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>event_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1545</td>\n",
       "      <td>4.0</td>\n",
       "      <td>978824139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>527</td>\n",
       "      <td>5.0</td>\n",
       "      <td>978824195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>745</td>\n",
       "      <td>3.0</td>\n",
       "      <td>978824268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating   event_ts\n",
       "0     1  1545     4.0  978824139\n",
       "1     1   527     5.0  978824195\n",
       "2     1   745     3.0  978824268"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: prepare the data for gmf model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 1.96 s, total: 1min 15s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = Data(\n",
    "    train=train,\n",
    "    test=test,\n",
    "    col_user='user',\n",
    "    col_item='item',\n",
    "    col_rating='rating',\n",
    "    col_time='event_ts',\n",
    "    binary=True,\n",
    "    n_neg=4,\n",
    "    n_neg_test=100,\n",
    "    n_neg_limit=0\n",
    ")\n",
    "dataset.prepTrainDNN(negSample=True)\n",
    "dataset.prepTestDNN(group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item_interacted</th>\n",
       "      <th>item_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>{40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{7, 11, 16, 17, 20, 28, 36, 40, 41, 42, 43, 44...</td>\n",
       "      <td>{0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{130, 131, 132, 133, 134, 135, 136, 9, 137, 13...</td>\n",
       "      <td>{0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                                    item_interacted  \\\n",
       "0     0  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1     1  {7, 11, 16, 17, 20, 28, 36, 40, 41, 42, 43, 44...   \n",
       "2     2  {130, 131, 132, 133, 134, 135, 136, 9, 137, 13...   \n",
       "\n",
       "                                       item_negative  \n",
       "0  {40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5...  \n",
       "1  {0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15...  \n",
       "2  {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.interaction_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newItems = set(dataset.items_test)-set(dataset.items)\n",
    "idx2del = []\n",
    "for idx,item in enumerate(dataset.items_test):\n",
    "    if item in newItems:\n",
    "        idx2del.append(idx)\n",
    "\n",
    "length_test_original = len(dataset.users_test)\n",
    "dataset.users_test = [\n",
    "    dataset.users_test[idx]\n",
    "    for idx in range(length_test_original) if idx not in idx2del\n",
    "]\n",
    "dataset.items_test = [\n",
    "    dataset.items_test[idx]\n",
    "    for idx in range(length_test_original) if idx not in idx2del\n",
    "]\n",
    "dataset.ratings_test = [\n",
    "    dataset.ratings_test[idx]\n",
    "    for idx in range(length_test_original) if idx not in idx2del\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: create the model architecture and fit model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3660)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.user.nunique(), train.item.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_users = 6040\n",
    "n_items = 3660\n",
    "# n_factors_gmf = 8\n",
    "# layers_mlp = [64,32,16,8]\n",
    "n_factors_gmf = 4\n",
    "layers_mlp = [16,8,4]\n",
    "reg_gmf = 0.\n",
    "reg_layers_mlp = [0.,0.,0.,0.]\n",
    "learning_rate = 0.001\n",
    "flg_pretrain = ''\n",
    "filepath = ''\n",
    "filepath_mlp_pretrain = ''\n",
    "filepath_mlp_pretrain = ''\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "ncf = NCF(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_factors_gmf=n_factors_gmf,\n",
    "    layers_mlp=layers_mlp,\n",
    "    reg_gmf=reg_gmf,\n",
    "    reg_layers_mlp=reg_layers_mlp\n",
    ")\n",
    "ncf.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf.compile(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_mlp_User (Embedding)  (None, 1, 8)         48320       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_mlp_Item (Embedding)  (None, 1, 8)         29280       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_mlp_User (Flatten)      (None, 8)            0           embedding_mlp_User[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_mlp_Item (Flatten)      (None, 8)            0           embedding_mlp_Item[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_gmf_User (Embedding)  (None, 1, 4)         24160       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_gmf_Item (Embedding)  (None, 1, 4)         14640       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concat_mlp_UserItem (Concatenat (None, 16)           0           flatten_mlp_User[0][0]           \n",
      "                                                                 flatten_mlp_Item[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_gmf_User (Flatten)      (None, 4)            0           embedding_gmf_User[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_gmf_Item (Flatten)      (None, 4)            0           embedding_gmf_Item[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "mlp_layer_1 (Dense)             (None, 8)            136         concat_mlp_UserItem[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply_gmf_UserItem (Multiply (None, 4)            0           flatten_gmf_User[0][0]           \n",
      "                                                                 flatten_gmf_Item[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mlp_layer_2 (Dense)             (None, 4)            36          mlp_layer_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat_gmf_mlp (Concatenate)    (None, 8)            0           multiply_gmf_UserItem[0][0]      \n",
      "                                                                 mlp_layer_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            9           concat_gmf_mlp[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 116,581\n",
      "Trainable params: 116,581\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ncf.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750605 samples, validate on 25252525 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16500, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3/ncf-weights-improvement-01-0.1650.hdf5\n",
      "3750605/3750605 - 106s - loss: 0.3394 - accuracy: 0.8484 - val_loss: 0.1650 - val_accuracy: 0.9430\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16500 to 0.15292, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3/ncf-weights-improvement-02-0.1529.hdf5\n",
      "3750605/3750605 - 101s - loss: 0.2934 - accuracy: 0.8704 - val_loss: 0.1529 - val_accuracy: 0.9448\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15292 to 0.14427, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3/ncf-weights-improvement-03-0.1443.hdf5\n",
      "3750605/3750605 - 106s - loss: 0.2822 - accuracy: 0.8757 - val_loss: 0.1443 - val_accuracy: 0.9477\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14427\n",
      "3750605/3750605 - 103s - loss: 0.2771 - accuracy: 0.8783 - val_loss: 0.1486 - val_accuracy: 0.9456\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.14427\n",
      "3750605/3750605 - 105s - loss: 0.2739 - accuracy: 0.8800 - val_loss: 0.1504 - val_accuracy: 0.9448\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.14427\n",
      "3750605/3750605 - 105s - loss: 0.2714 - accuracy: 0.8812 - val_loss: 0.1502 - val_accuracy: 0.9432\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14427\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "3750605/3750605 - 109s - loss: 0.2693 - accuracy: 0.8823 - val_loss: 0.1454 - val_accuracy: 0.9461\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14427\n",
      "3750605/3750605 - 101s - loss: 0.2618 - accuracy: 0.8861 - val_loss: 0.1445 - val_accuracy: 0.9462\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14427 to 0.14385, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3/ncf-weights-improvement-09-0.1439.hdf5\n",
      "3750605/3750605 - 98s - loss: 0.2602 - accuracy: 0.8870 - val_loss: 0.1439 - val_accuracy: 0.9463\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.14385 to 0.14348, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3/ncf-weights-improvement-10-0.1435.hdf5\n",
      "3750605/3750605 - 98s - loss: 0.2594 - accuracy: 0.8873 - val_loss: 0.1435 - val_accuracy: 0.9458\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14348\n",
      "3750605/3750605 - 101s - loss: 0.2589 - accuracy: 0.8876 - val_loss: 0.1442 - val_accuracy: 0.9458\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14348\n",
      "3750605/3750605 - 103s - loss: 0.2584 - accuracy: 0.8879 - val_loss: 0.1472 - val_accuracy: 0.9439\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14348\n",
      "3750605/3750605 - 102s - loss: 0.2580 - accuracy: 0.8880 - val_loss: 0.1499 - val_accuracy: 0.9425\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14348 to 0.14298, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3/ncf-weights-improvement-14-0.1430.hdf5\n",
      "3750605/3750605 - 98s - loss: 0.2576 - accuracy: 0.8883 - val_loss: 0.1430 - val_accuracy: 0.9458\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14298\n",
      "3750605/3750605 - 101s - loss: 0.2573 - accuracy: 0.8884 - val_loss: 0.1518 - val_accuracy: 0.9417\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.14298\n",
      "3750605/3750605 - 99s - loss: 0.2569 - accuracy: 0.8886 - val_loss: 0.1448 - val_accuracy: 0.9446\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14298\n",
      "3750605/3750605 - 99s - loss: 0.2566 - accuracy: 0.8888 - val_loss: 0.1490 - val_accuracy: 0.9431\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14298\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "3750605/3750605 - 101s - loss: 0.2563 - accuracy: 0.8890 - val_loss: 0.1540 - val_accuracy: 0.9402\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.14298\n",
      "3750605/3750605 - 99s - loss: 0.2533 - accuracy: 0.8905 - val_loss: 0.1492 - val_accuracy: 0.9426\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.14298\n",
      "3750605/3750605 - 98s - loss: 0.2531 - accuracy: 0.8906 - val_loss: 0.1479 - val_accuracy: 0.9430\n"
     ]
    }
   ],
   "source": [
    "hist = ncf.fit(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    path_model_weights='/Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3/ncf-weights-improvement-{epoch:02d}-{val_loss:.4f}.hdf5',\n",
    "    path_csvlog='/Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3/ncf_log.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to load the parameters from the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf.model.load_weights('../metadata/ncf3/ncf-weights-improvement-14-0.1430.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ncf.model.evaluate(\n",
    "    x = [\n",
    "        np.array(dataset.users_test),\n",
    "        np.array(dataset.items_test)\n",
    "    ],\n",
    "    y = np.array(dataset.ratings_test),\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1429816370327044, 0.94643754]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf.model.save('../metadata/ncf3/ncf-best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.models.load_model('../metadata/ncf3/ncf-best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [09:17<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 32s, sys: 2min 41s, total: 24min 14s\n",
      "Wall time: 17min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.20243901093434202, 0.8561481331307735, 0.1429816375232299)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "evaluator = evaluation_grouped.metricsEval(\n",
    "    model=model3,\n",
    "    users=dataset.users,\n",
    "    items=dataset.items\n",
    ")\n",
    "evaluator.getRecs()\n",
    "rmse,auc,logloss = evaluator.getOverlapBasedMetrics(\n",
    "    dataset.users_test,\n",
    "    dataset.items_test,\n",
    "    dataset.ratings_test\n",
    ")\n",
    "rmse,auc,logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02646146203922253,\n",
       " 0.04362582781456954,\n",
       " 0.044529338374450375,\n",
       " 0.008927550968871921)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall,precision,ndcg,map2 = evaluator.getRankBasedMetrics(\n",
    "    dataset.users_test,\n",
    "    dataset.items_test,\n",
    "    dataset.ratings_test\n",
    ")\n",
    "recall,precision,ndcg,map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750605 samples, validate on 25252525 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14317, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3_2/ncf-weights-improvement-01-0.1432.hdf5\n",
      "3750605/3750605 - 100s - loss: 0.2546 - accuracy: 0.8897 - val_loss: 0.1432 - val_accuracy: 0.9458\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 100s - loss: 0.2544 - accuracy: 0.8899 - val_loss: 0.1462 - val_accuracy: 0.9442\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 100s - loss: 0.2542 - accuracy: 0.8900 - val_loss: 0.1445 - val_accuracy: 0.9448\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 99s - loss: 0.2541 - accuracy: 0.8900 - val_loss: 0.1482 - val_accuracy: 0.9434\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.14317\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "3750605/3750605 - 99s - loss: 0.2540 - accuracy: 0.8901 - val_loss: 0.1467 - val_accuracy: 0.9438\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 99s - loss: 0.2529 - accuracy: 0.8906 - val_loss: 0.1457 - val_accuracy: 0.9442\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 103s - loss: 0.2529 - accuracy: 0.8906 - val_loss: 0.1465 - val_accuracy: 0.9438\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 105s - loss: 0.2528 - accuracy: 0.8907 - val_loss: 0.1469 - val_accuracy: 0.9436\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14317\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
      "3750605/3750605 - 106s - loss: 0.2528 - accuracy: 0.8907 - val_loss: 0.1470 - val_accuracy: 0.9434\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 108s - loss: 0.2524 - accuracy: 0.8909 - val_loss: 0.1470 - val_accuracy: 0.9435\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 100s - loss: 0.2524 - accuracy: 0.8909 - val_loss: 0.1460 - val_accuracy: 0.9440\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14317\n",
      "3750605/3750605 - 99s - loss: 0.2524 - accuracy: 0.8909 - val_loss: 0.1470 - val_accuracy: 0.9434\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14317\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-06.\n",
      "3750605/3750605 - 99s - loss: 0.2524 - accuracy: 0.8909 - val_loss: 0.1471 - val_accuracy: 0.9434\n"
     ]
    }
   ],
   "source": [
    "hist = ncf.fit(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    path_model_weights='/Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3_2/ncf-weights-improvement-{epoch:02d}-{val_loss:.4f}.hdf5',\n",
    "    path_csvlog='/Users/xyin/Documents/work/projects/rec_utils/metadata/ncf3_2/ncf_log.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
