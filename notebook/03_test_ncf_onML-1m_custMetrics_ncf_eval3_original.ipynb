{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the NCF module under folder [cf_ec2](../cf_ec2) with ml-1m dataset, save the best model (using integrated modules with compile and fit components, with gmf and mlp pretrain)\n",
    "\n",
    "#### 4/20/2020, test with original paper's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import (\n",
    "    Adam,\n",
    "    Adamax,\n",
    "    Adagrad,\n",
    "    SGD,\n",
    "    RMSprop\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, \n",
    "    Input,\n",
    "    Flatten, \n",
    "    Multiply, \n",
    "    Concatenate,\n",
    "    Dense\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from cf_ec2 import (\n",
    "    GMF,\n",
    "    MLP,\n",
    "    NCF,\n",
    "    Data,\n",
    "    evaluation,\n",
    "    evaluation_grouped\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check original paper's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../metadata/original_dataset/\u001b[00m\r\n",
      "├── item_input\r\n",
      "├── labels\r\n",
      "├── testNegatives\r\n",
      "├── testRatings\r\n",
      "├── train\r\n",
      "└── user_input\r\n",
      "\r\n",
      "0 directories, 6 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../metadata/original_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../metadata/original_dataset/train','rb') as fp:\n",
    "    train = pickle.load(fp, encoding='latin1')\n",
    "with open('../metadata/original_dataset/testRatings','rb') as fp:\n",
    "    testRatings = pickle.load(fp, encoding='latin1')\n",
    "with open('../metadata/original_dataset/testNegatives','rb') as fp:\n",
    "    testNegatives = pickle.load(fp, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse.dok.dok_matrix, list, list)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train), type(testRatings), type(testNegatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 25], [1, 133], [2, 207], [3, 208], [4, 222]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRatings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 6040)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testRatings), len(testNegatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../metadata/original_dataset/user_input','rb') as fp:\n",
    "    user_input = pickle.load(fp, encoding='latin1')\n",
    "with open('../metadata/original_dataset/item_input','rb') as fp:\n",
    "    item_input = pickle.load(fp, encoding='latin1')\n",
    "with open('../metadata/original_dataset/labels','rb') as fp:\n",
    "    labels = pickle.load(fp, encoding='latin1')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, list)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_input), type(item_input), type(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reformat the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [0, 25] //[1064, 174, 2791, 3373, 269]\n",
      "1: [1, 133] //[1072, 3154, 3368, 3644, 549]\n",
      "2: [2, 207] //[2216, 209, 2347, 3, 1652]\n",
      "3: [3, 208] //[3023, 1489, 1916, 1706, 1221]\n",
      "4: [4, 222] //[1794, 3535, 108, 593, 466]\n"
     ]
    }
   ],
   "source": [
    "for idx,value in enumerate(testRatings):\n",
    "    if idx<5:\n",
    "        print('{}: {} //{}'.format(idx,value, testNegatives[idx][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test, item_test, labels_test = [],[],[]\n",
    "for idx in range(len(testRatings)):\n",
    "    user_test.extend(\n",
    "        [testRatings[idx][0]]*(len(testNegatives[idx])+1)\n",
    "    )\n",
    "    item_test.append(testRatings[idx][1])\n",
    "    item_test.extend(testNegatives[idx])\n",
    "    labels_test.append(1)\n",
    "    labels_test.extend([0]*len(testNegatives[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: create the model architecture and fit model with dataset from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users, num_items = train.shape\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_users = num_users\n",
    "n_items = num_items\n",
    "n_factors_gmf = 8\n",
    "layers_mlp = [64,32,16,8]\n",
    "# n_factors_gmf = 4\n",
    "# layers_mlp = [16,8,4]\n",
    "reg_gmf = 0.\n",
    "reg_layers_mlp = [0.,0.,0.,0.]\n",
    "learning_rate = 0.001\n",
    "flg_pretrain = ''\n",
    "filepath = ''\n",
    "filepath_mlp_pretrain = ''\n",
    "filepath_mlp_pretrain = ''\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "ncf = NCF(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_factors_gmf=n_factors_gmf,\n",
    "    layers_mlp=layers_mlp,\n",
    "    reg_gmf=reg_gmf,\n",
    "    reg_layers_mlp=reg_layers_mlp\n",
    ")\n",
    "ncf.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf.compile(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_mlp_User (Embedding)  (None, 1, 32)        193280      user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_mlp_Item (Embedding)  (None, 1, 32)        118592      item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_mlp_User (Flatten)      (None, 32)           0           embedding_mlp_User[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_mlp_Item (Flatten)      (None, 32)           0           embedding_mlp_Item[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concat_mlp_UserItem (Concatenat (None, 64)           0           flatten_mlp_User[0][0]           \n",
      "                                                                 flatten_mlp_Item[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_gmf_User (Embedding)  (None, 1, 8)         48320       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_gmf_Item (Embedding)  (None, 1, 8)         29648       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlp_layer_1 (Dense)             (None, 32)           2080        concat_mlp_UserItem[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_gmf_User (Flatten)      (None, 8)            0           embedding_gmf_User[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_gmf_Item (Flatten)      (None, 8)            0           embedding_gmf_Item[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "mlp_layer_2 (Dense)             (None, 16)           528         mlp_layer_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_gmf_UserItem (Multiply (None, 8)            0           flatten_gmf_User[0][0]           \n",
      "                                                                 flatten_gmf_Item[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mlp_layer_3 (Dense)             (None, 8)            136         mlp_layer_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat_gmf_mlp (Concatenate)    (None, 16)           0           multiply_gmf_UserItem[0][0]      \n",
      "                                                                 mlp_layer_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            17          concat_gmf_mlp[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 392,601\n",
      "Trainable params: 392,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ncf.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4970845 samples, validate on 604000 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16958, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf4/ncf-weights-improvement-01-0.1696.hdf5\n",
      "4970845/4970845 - 57s - loss: 0.3219 - accuracy: 0.8570 - val_loss: 0.1696 - val_accuracy: 0.9391\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16958 to 0.15503, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf4/ncf-weights-improvement-02-0.1550.hdf5\n",
      "4970845/4970845 - 56s - loss: 0.2744 - accuracy: 0.8802 - val_loss: 0.1550 - val_accuracy: 0.9410\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15503 to 0.15225, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf4/ncf-weights-improvement-03-0.1523.hdf5\n",
      "4970845/4970845 - 53s - loss: 0.2627 - accuracy: 0.8858 - val_loss: 0.1523 - val_accuracy: 0.9412\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15225 to 0.14039, saving model to /Users/xyin/Documents/work/projects/rec_utils/metadata/ncf4/ncf-weights-improvement-04-0.1404.hdf5\n",
      "4970845/4970845 - 53s - loss: 0.2559 - accuracy: 0.8892 - val_loss: 0.1404 - val_accuracy: 0.9454\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 53s - loss: 0.2508 - accuracy: 0.8917 - val_loss: 0.1588 - val_accuracy: 0.9372\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 55s - loss: 0.2466 - accuracy: 0.8940 - val_loss: 0.1514 - val_accuracy: 0.9405\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 53s - loss: 0.2430 - accuracy: 0.8957 - val_loss: 0.1471 - val_accuracy: 0.9424\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14039\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "4970845/4970845 - 54s - loss: 0.2397 - accuracy: 0.8973 - val_loss: 0.1455 - val_accuracy: 0.9443\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 53s - loss: 0.2273 - accuracy: 0.9036 - val_loss: 0.1502 - val_accuracy: 0.9404\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 54s - loss: 0.2242 - accuracy: 0.9052 - val_loss: 0.1587 - val_accuracy: 0.9367\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 54s - loss: 0.2225 - accuracy: 0.9060 - val_loss: 0.1585 - val_accuracy: 0.9375\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14039\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "4970845/4970845 - 57s - loss: 0.2212 - accuracy: 0.9068 - val_loss: 0.1449 - val_accuracy: 0.9430\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 54s - loss: 0.2157 - accuracy: 0.9095 - val_loss: 0.1523 - val_accuracy: 0.9401\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 53s - loss: 0.2150 - accuracy: 0.9098 - val_loss: 0.1524 - val_accuracy: 0.9400\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14039\n",
      "4970845/4970845 - 51s - loss: 0.2144 - accuracy: 0.9102 - val_loss: 0.1508 - val_accuracy: 0.9405\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.14039\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "4970845/4970845 - 52s - loss: 0.2140 - accuracy: 0.9104 - val_loss: 0.1566 - val_accuracy: 0.9383\n"
     ]
    }
   ],
   "source": [
    "# def fit(self, dataset, \n",
    "batch_size=256\n",
    "num_epochs=20\n",
    "path_model_weights='/Users/xyin/Documents/work/projects/rec_utils/metadata/ncf4/ncf-weights-improvement-{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "path_csvlog='/Users/xyin/Documents/work/projects/rec_utils/metadata/ncf4/ncf_log.csv'\n",
    "\n",
    "## create the callback metrics\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath= path_model_weights, \n",
    "    monitor='val_loss',\n",
    "    verbose=1, \n",
    "    save_best_only=True\n",
    ")\n",
    "csvlog = tf.keras.callbacks.CSVLogger(\n",
    "    filename=path_csvlog, \n",
    "    separator=',', \n",
    "    append=False\n",
    ")\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(patience=12)\n",
    "lrreduce = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", \n",
    "    factor=0.3, \n",
    "    patience=4, \n",
    "    verbose=1\n",
    ")  \n",
    "metrics2 = evaluation_grouped.metricsCallback(batch_size=batch_size,log_steps=100)      \n",
    "## fit the model\n",
    "hist = ncf.model.fit(\n",
    "    x = [\n",
    "        np.array(user_input),\n",
    "        np.array(item_input)\n",
    "    ],\n",
    "    y = np.array(labels),\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    verbose=2,\n",
    "    shuffle=True,\n",
    "    callbacks=[metrics2,checkpoint,csvlog,earlystop,lrreduce],\n",
    "    validation_data=(\n",
    "        [\n",
    "            np.array(user_test),\n",
    "            np.array(item_test)\n",
    "        ],\n",
    "        np.array(labels_test)\n",
    "    )\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to load the parameters from the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf.model.load_weights('../metadata/ncf4/ncf-weights-improvement-04-0.1404.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 2.44 s, total: 19.2 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "scores = ncf.model.evaluate(\n",
    "    x = [\n",
    "        np.array(user_test),\n",
    "        np.array(item_test)\n",
    "    ],\n",
    "    y = np.array(labels_test),\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14038968149246955, 0.945404]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf.model.save('../metadata/ncf4/ncf-best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.models.load_model('../metadata/ncf4/ncf-best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [09:03<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 20s, sys: 1min 22s, total: 12min 43s\n",
      "Wall time: 9min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2016294613947125, 0.8940297085975318, 0.1403896816127934)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "evaluator = evaluation_grouped.metricsEval(\n",
    "    model=model4,\n",
    "    users=user_input,\n",
    "    items=item_input\n",
    ")\n",
    "evaluator.getRecs()\n",
    "rmse,auc,logloss = evaluator.getOverlapBasedMetrics(\n",
    "    user_test,\n",
    "    item_test,\n",
    "    labels_test\n",
    ")\n",
    "rmse,auc,logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.7 s, sys: 8.58 s, total: 59.3 s\n",
      "Wall time: 1min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.029635761589403974,\n",
       " 0.002963576158940397,\n",
       " 0.013684980463253551,\n",
       " 0.008958596131609377)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "recall,precision,ndcg,map2 = evaluator.getRankBasedMetrics(\n",
    "    user_test,\n",
    "    item_test,\n",
    "    labels_test\n",
    ")\n",
    "recall,precision,ndcg,map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics are still one order lower than reported numbers in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
